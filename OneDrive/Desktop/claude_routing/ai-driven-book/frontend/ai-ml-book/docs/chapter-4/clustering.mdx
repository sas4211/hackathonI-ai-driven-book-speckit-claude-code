---
sidebar_position: 2
---

# Clustering Fundamentals

Clustering is the process of grouping similar data points together while separating dissimilar ones. It's one of the most intuitive and widely used techniques in unsupervised learning, allowing us to discover hidden patterns and structures in data without predefined labels.

## What is Clustering?

Clustering algorithms partition a dataset into groups (clusters) where:
- **Within-cluster similarity**: Data points in the same cluster are similar to each other
- **Between-cluster dissimilarity**: Data points in different clusters are dissimilar

### Why Clustering Matters

Clustering helps us:
- **Discover structure** in unlabeled data
- **Simplify complex datasets** by grouping similar items
- **Identify patterns** that might not be immediately obvious
- **Support decision-making** in various domains

## Types of Clustering Algorithms

### 1. Partitioning Clustering
- **K-Means**: Partitions data into K spherical clusters
- **K-Medoids**: Similar to K-Means but uses actual data points as centers
- **Advantages**: Simple, fast, works well with spherical clusters

### 2. Hierarchical Clustering
- **Agglomerative**: Bottom-up approach (merge clusters)
- **Divisive**: Top-down approach (split clusters)
- **Advantages**: No need to specify number of clusters, produces dendrograms

### 3. Density-Based Clustering
- **DBSCAN**: Groups points in high-density regions
- **OPTICS**: Extension of DBSCAN for varying densities
- **Advantages**: Handles noise, finds arbitrary-shaped clusters

### 4. Distribution-Based Clustering
- **Gaussian Mixture Models**: Assumes data comes from mixture of Gaussian distributions
- **Advantages**: Probabilistic assignments, handles overlapping clusters

### 5. Spectral Clustering
- Uses eigenvalues of similarity matrices
- **Advantages**: Good for non-convex clusters

## Distance and Similarity Measures

### Euclidean Distance
The most common distance measure:
```
d(x,y) = √(Σ(xi - yi)²)
```

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.metrics.pairwise import euclidean_distances

# Generate sample data
X, y_true = make_blobs(n_samples=100, centers=3, random_state=42, cluster_std=1.0)

# Calculate pairwise distances
distances = euclidean_distances(X[:10])  # First 10 points

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.7)
plt.title('Sample Data Points')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

plt.subplot(1, 2, 2)
plt.imshow(distances, cmap='viridis')
plt.colorbar(label='Euclidean Distance')
plt.title('Distance Matrix (First 10 Points)')
plt.xlabel('Point Index')
plt.ylabel('Point Index')

plt.tight_layout()
plt.show()

print("Distance matrix shape:", distances.shape)
print("Sample distances:")
print(distances[:3, :3])  # Show first 3x3 submatrix
```

### Manhattan Distance (L1 Norm)
Sum of absolute differences:
```
d(x,y) = Σ|xi - yi|
```

```python
from sklearn.metrics.pairwise import manhattan_distances

manhattan_dist = manhattan_distances(X[:10])

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.imshow(manhattan_dist, cmap='coolwarm')
plt.colorbar(label='Manhattan Distance')
plt.title('Manhattan Distance Matrix')

plt.subplot(1, 2, 2)
# Compare with Euclidean
euclidean_dist = euclidean_distances(X[:10])
plt.scatter(euclidean_dist.flatten(), manhattan_dist.flatten(), alpha=0.6)
plt.xlabel('Euclidean Distance')
plt.ylabel('Manhattan Distance')
plt.title('Euclidean vs Manhattan Distance')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Cosine Similarity
Measures angle between vectors (ignores magnitude):
```
cos(θ) = (A·B) / (||A|| ||B||)
```

```python
from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(X[:10])

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.imshow(cosine_sim, cmap='RdYlBu', vmin=-1, vmax=1)
plt.colorbar(label='Cosine Similarity')
plt.title('Cosine Similarity Matrix')

plt.subplot(1, 3, 2)
# Convert to distance
cosine_dist = 1 - cosine_sim
plt.imshow(cosine_dist, cmap='viridis')
plt.colorbar(label='Cosine Distance')
plt.title('Cosine Distance Matrix')

plt.subplot(1, 3, 3)
# Compare all three distance measures
euclidean_norm = euclidean_dist / euclidean_dist.max()
manhattan_norm = manhattan_dist / manhattan_dist.max()
cosine_norm = cosine_dist / cosine_dist.max()

plt.plot(euclidean_norm[0], label='Euclidean', linewidth=2)
plt.plot(manhattan_norm[0], label='Manhattan', linewidth=2)
plt.plot(cosine_norm[0], label='Cosine', linewidth=2)
plt.xlabel('Point Index')
plt.ylabel('Normalized Distance')
plt.title('Distance Measure Comparison')
plt.legend()

plt.tight_layout()
plt.show()
```

## Basic Clustering Example

Let's explore clustering with a simple example:

```python
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import adjusted_rand_score, silhouette_score

# Generate diverse datasets
np.random.seed(42)

# 1. Spherical clusters (good for K-Means)
X_spherical, y_spherical = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)

# 2. Moon-shaped clusters (challenging for K-Means)
from sklearn.datasets import make_moons
X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)

# 3. Circle clusters (challenging for most algorithms)
from sklearn.datasets import make_circles
X_circles, y_circles = make_circles(n_samples=200, noise=0.05, factor=0.5, random_state=42)

datasets = [
    ("Spherical Clusters", X_spherical, y_spherical, 4),
    ("Moon Clusters", X_moons, y_moons, 2),
    ("Circle Clusters", X_circles, y_circles, 2)
]

# Clustering algorithms to compare
algorithms = [
    ("K-Means", KMeans(n_clusters=4, random_state=42, n_init=10)),
    ("Hierarchical", AgglomerativeClustering(n_clusters=4)),
    ("DBSCAN", DBSCAN(eps=0.3, min_samples=5))
]

plt.figure(figsize=(20, 15))

for i, (name, X, y, n_clusters) in enumerate(datasets):
    plt.subplot(len(datasets), len(algorithms) + 1, i * (len(algorithms) + 1) + 1)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7, s=30)
    plt.title(f'True Labels\n{name}')
    plt.xticks([])
    plt.yticks([])

    for j, (algo_name, algorithm) in enumerate(algorithms):
        # Adjust algorithm parameters for different datasets
        if algo_name == "K-Means":
            algorithm.n_clusters = n_clusters
        elif algo_name == "Hierarchical":
            algorithm.n_clusters = n_clusters
        elif algo_name == "DBSCAN":
            if name == "Spherical Clusters":
                algorithm.eps = 0.5
            elif name == "Moon Clusters":
                algorithm.eps = 0.3
            else:
                algorithm.eps = 0.2

        # Fit and predict
        y_pred = algorithm.fit_predict(X)

        # Calculate metrics
        if len(np.unique(y_pred)) > 1:  # Only if clustering succeeded
            ari = adjusted_rand_score(y, y_pred)
            silhouette = silhouette_score(X, y_pred)
        else:
            ari = 0
            silhouette = 0

        # Plot results
        plt.subplot(len(datasets), len(algorithms) + 1, i * (len(algorithms) + 1) + j + 2)
        scatter = plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.7, s=30)
        plt.title(f'{algo_name}\nARI: {ari:.2f}, Sil: {silhouette:.2f}')
        plt.xticks([])
        plt.yticks([])

plt.tight_layout()
plt.show()
```

## Clustering Quality Assessment

### Internal Validation Metrics

These metrics evaluate clustering quality without ground truth:

```python
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# Generate test data
X_test, y_test = make_blobs(n_samples=300, centers=5, cluster_std=1.0, random_state=42)

# Test different numbers of clusters
k_range = range(2, 11)
silhouette_scores = []
calinski_harabasz_scores = []
davies_bouldin_scores = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_test)

    # Calculate metrics
    silhouette_scores.append(silhouette_score(X_test, labels))
    calinski_harabasz_scores.append(calinski_harabasz_score(X_test, labels))
    davies_bouldin_scores.append(davies_bouldin_score(X_test, labels))

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(k_range, silhouette_scores, 'bo-')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score vs K')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
plt.plot(k_range, calinski_harabasz_scores, 'ro-')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Calinski-Harabasz Score')
plt.title('Calinski-Harabasz Score vs K')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
plt.plot(k_range, davies_bouldin_scores, 'go-')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Davies-Bouldin Score')
plt.title('Davies-Bouldin Score vs K')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Find optimal K
best_silhouette_k = k_range[np.argmax(silhouette_scores)]
best_ch_k = k_range[np.argmax(calinski_harabasz_scores)]
best_db_k = k_range[np.argmin(davies_bouldin_scores)]

print(f"Optimal K by Silhouette Score: {best_silhouette_k}")
print(f"Optimal K by Calinski-Harabasz Score: {best_ch_k}")
print(f"Optimal K by Davies-Bouldin Score: {best_db_k}")
```

### External Validation Metrics

When ground truth is available:

```python
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_completeness_v_measure

# Generate data with known labels
X_known, y_known = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)

# Test different algorithms
algorithms = [
    ("K-Means", KMeans(n_clusters=4, random_state=42, n_init=10)),
    ("Hierarchical", AgglomerativeClustering(n_clusters=4)),
    ("DBSCAN", DBSCAN(eps=0.8, min_samples=10))
]

results = {}

for name, algorithm in algorithms:
    y_pred = algorithm.fit_predict(X_known)

    # Calculate external validation metrics
    ari = adjusted_rand_score(y_known, y_pred)
    nmi = normalized_mutual_info_score(y_known, y_pred)
    homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(y_known, y_pred)

    results[name] = {
        'ARI': ari,
        'NMI': nmi,
        'Homogeneity': homogeneity,
        'Completeness': completeness,
        'V-Measure': v_measure
    }

# Display results
results_df = pd.DataFrame(results).T
print("Clustering Algorithm Comparison:")
print("=" * 50)
print(results_df.round(3))

# Visualize results
plt.figure(figsize=(15, 10))

for i, (name, algorithm) in enumerate(algorithms):
    y_pred = algorithm.fit_predict(X_known)

    plt.subplot(2, 3, i+1)
    plt.scatter(X_known[:, 0], X_known[:, 1], c=y_pred, cmap='viridis', alpha=0.7, s=30)
    plt.title(f'{name}\nARI: {results[name]["ARI"]:.3f}')

    plt.subplot(2, 3, i+4)
    plt.scatter(X_known[:, 0], X_known[:, 1], c=y_known, cmap='coolwarm', alpha=0.7, s=30)
    plt.title('True Labels')

plt.tight_layout()
plt.show()
```

## Advanced Clustering Concepts

### Cluster Stability

How consistent are clustering results across different runs or subsamples?

```python
from sklearn.model_selection import train_test_split

# Test cluster stability
X_stable, y_stable = make_blobs(n_samples=200, centers=3, cluster_std=1.0, random_state=42)

stability_scores = []

for i in range(10):
    # Split data differently each time
    X_train, X_test, y_train, y_test = train_test_split(X_stable, y_stable, test_size=0.5, random_state=i)

    # Fit on training set
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    labels_train = kmeans.fit_predict(X_train)

    # Predict on test set
    labels_test = kmeans.predict(X_test)

    # Calculate stability
    stability = adjusted_rand_score(y_test, labels_test)
    stability_scores.append(stability)

print(f"Average stability score: {np.mean(stability_scores):.3f} ± {np.std(stability_scores):.3f}")

plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), stability_scores, 'bo-')
plt.axhline(y=np.mean(stability_scores), color='red', linestyle='--', label=f'Mean: {np.mean(stability_scores):.3f}')
plt.xlabel('Run Number')
plt.ylabel('Stability Score (ARI)')
plt.title('Clustering Stability Across Different Data Splits')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Feature Importance in Clustering

Which features contribute most to cluster formation?

```python
# Generate data with informative and non-informative features
np.random.seed(42)
n_samples = 300

# Informative features
X_informative = make_blobs(n_samples=n_samples, centers=3, n_features=2, cluster_std=1.0, random_state=42)[0]

# Non-informative features (random noise)
X_noise = np.random.normal(0, 1, (n_samples, 5))

# Combine features
X_mixed = np.hstack([X_informative, X_noise])

# Apply clustering
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_mixed)

# Feature importance analysis
feature_importance = []

for i in range(X_mixed.shape[1]):
    # Cluster using only feature i
    X_single = X_mixed[:, i].reshape(-1, 1)
    kmeans_single = KMeans(n_clusters=3, random_state=42, n_init=10)
    labels_single = kmeans_single.fit_predict(X_single)

    # Compare with full clustering
    importance = adjusted_rand_score(labels, labels_single)
    feature_importance.append(importance)

# Visualize feature importance
feature_names = [f'Feature {i+1}' for i in range(X_mixed.shape[1])]
feature_names[0] = 'Informative 1'
feature_names[1] = 'Informative 2'
for i in range(2, len(feature_names)):
    feature_names[i] = f'Noise {i-1}'

plt.figure(figsize=(12, 6))
plt.bar(range(len(feature_importance)), feature_importance)
plt.xlabel('Feature')
plt.ylabel('Importance (ARI with full clustering)')
plt.title('Feature Importance in Clustering')
plt.xticks(range(len(feature_importance)), feature_names, rotation=45)
plt.grid(True, alpha=0.3)
plt.show()

print("Feature importance scores:")
for i, (name, importance) in enumerate(zip(feature_names, feature_importance)):
    print(f"  {name}: {importance:.3f}")
```

## Real-World Clustering Applications

### Customer Segmentation

```python
# Simulate customer data
np.random.seed(123)

# Customer features
n_customers = 1000
shopping_frequency = np.random.exponential(3, n_customers)
avg_spending = np.random.normal(100, 50, n_customers)
product_variety = np.random.poisson(5, n_customers)
customer_age = np.random.normal(35, 12, n_customers)

# Create customer profiles
customer_data = pd.DataFrame({
    'Shopping_Frequency': shopping_frequency,
    'Avg_Spending': avg_spending,
    'Product_Variety': product_variety,
    'Customer_Age': customer_age
})

# Normalize features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_customers = scaler.fit_transform(customer_data)

# Apply clustering
kmeans_customers = KMeans(n_clusters=4, random_state=42, n_init=10)
customer_segments = kmeans_customers.fit_predict(X_customers)

# Add segments to data
customer_data['Segment'] = customer_segments

# Analyze segments
plt.figure(figsize=(15, 10))

features = ['Shopping_Frequency', 'Avg_Spending', 'Product_Variety', 'Customer_Age']
for i, feature in enumerate(features):
    plt.subplot(2, 2, i+1)
    for segment in range(4):
        segment_data = customer_data[customer_data['Segment'] == segment][feature]
        plt.hist(segment_data, alpha=0.6, label=f'Segment {segment}', bins=20)
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.title(f'{feature} by Customer Segment')
    plt.legend()

plt.tight_layout()
plt.show()

# Segment profiles
print("Customer Segment Analysis:")
print("=" * 50)
for segment in range(4):
    segment_data = customer_data[customer_data['Segment'] == segment]
    print(f"\nSegment {segment} ({len(segment_data)} customers):")
    print(f"  Shopping Frequency: {segment_data['Shopping_Frequency'].mean():.1f}")
    print(f"  Avg Spending: ${segment_data['Avg_Spending'].mean():.1f}")
    print(f"  Product Variety: {segment_data['Product_Variety'].mean():.1f}")
    print(f"  Customer Age: {segment_data['Customer_Age'].mean():.1f}")
```

### Document Clustering

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

# Sample documents
documents = [
    "Machine learning is a subset of artificial intelligence",
    "Deep learning uses neural networks with many layers",
    "Natural language processing helps computers understand text",
    "Computer vision enables machines to interpret images",
    "Data science combines statistics and programming",
    "Python is popular for data analysis and machine learning",
    "R programming is used for statistical analysis",
    "SQL is essential for database management",
    "Statistics provides tools for data analysis",
    "Mathematics forms the foundation of machine learning"
]

# Convert to TF-IDF vectors
vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
X_docs = vectorizer.fit_transform(documents)

# Reduce dimensions for visualization
svd = TruncatedSVD(n_components=2, random_state=42)
X_docs_2d = svd.fit_transform(X_docs)

# Apply clustering
kmeans_docs = KMeans(n_clusters=3, random_state=42, n_init=10)
doc_clusters = kmeans_docs.fit_predict(X_docs)

# Visualize
plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_docs_2d[:, 0], X_docs_2d[:, 1], c=doc_clusters, cmap='viridis', s=100)

# Add text labels
for i, doc in enumerate(documents):
    short_doc = doc[:30] + "..." if len(doc) > 30 else doc
    plt.annotate(f"{i+1}", (X_docs_2d[i, 0], X_docs_2d[i, 1]),
                xytext=(5, 5), textcoords='offset points', fontsize=8)

plt.colorbar(scatter)
plt.xlabel('First Component')
plt.ylabel('Second Component')
plt.title('Document Clustering Visualization')
plt.grid(True, alpha=0.3)
plt.show()

# Show clustering results
print("Document Clustering Results:")
print("=" * 40)
for cluster in range(3):
    cluster_docs = [i for i, label in enumerate(doc_clusters) if label == cluster]
    print(f"\nCluster {cluster}:")
    for doc_idx in cluster_docs:
        print(f"  {doc_idx+1}. {documents[doc_idx]}")
```

## Key Takeaways

1. **Choose the right algorithm**: Different algorithms work better for different data types and cluster shapes
2. **Distance matters**: The choice of distance metric significantly affects clustering results
3. **Validate your results**: Use multiple metrics to assess clustering quality
4. **Consider scalability**: Some algorithms don't scale well to large datasets
5. **Feature engineering is crucial**: Clustering quality depends heavily on feature representation
6. **Interpretability**: Always try to understand what your clusters represent in the real world
7. **Domain knowledge**: Incorporate domain expertise when choosing algorithms and interpreting results

Clustering is both an art and a science. While there are mathematical principles to guide your choices, experience and domain knowledge play crucial roles in achieving meaningful results.
