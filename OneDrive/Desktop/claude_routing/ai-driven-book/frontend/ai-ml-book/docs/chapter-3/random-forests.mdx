---
sidebar_position: 5
---

# Random Forests

Random Forests are one of the most powerful and versatile ensemble learning methods. They combine multiple decision trees to create a robust model that reduces overfitting and improves generalization through the power of collective intelligence.

## What are Random Forests?

Random Forests work by creating many decision trees (a "forest") and combining their predictions. Each tree is trained on a random subset of the data and features, then the final prediction is made by aggregating the predictions of all trees.

### The Wisdom of Crowds Principle

The core idea is that while individual decision trees may overfit or be unstable, a collection of diverse trees can produce more accurate and stable predictions.

**Key Components:**
- **Bootstrap Sampling**: Each tree gets a random sample of the training data
- **Feature Randomness**: Each split considers only a random subset of features
- **Ensemble Aggregation**: Final prediction combines all tree predictions

## How Random Forests Work

### 1. Bootstrap Aggregating (Bagging)

Each tree is trained on a different bootstrap sample:
- Randomly select samples from the original dataset **with replacement**
- Each sample has the same size as the original dataset
- Some samples will be repeated, others will be missing

### 2. Feature Randomness

At each split in each tree:
- Randomly select a subset of features (typically √p for classification, p/3 for regression)
- Find the best split among only those features
- This decorrelates the trees and reduces variance

### 3. Voting and Averaging

For predictions:
- **Classification**: Majority vote among all trees
- **Regression**: Average prediction of all trees

## Basic Implementation

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,
                          n_redundant=2, n_clusters_per_class=1, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Compare single tree vs random forest
single_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)

single_tree.fit(X_train, y_train)
random_forest.fit(X_train, y_train)

single_pred = single_tree.predict(X_test)
forest_pred = random_forest.predict(X_test)

print(f"Single Decision Tree Accuracy: {accuracy_score(y_test, single_pred):.3f}")
print(f"Random Forest Accuracy: {accuracy_score(y_test, forest_pred):.3f}")

# Visualize improvement
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(['Single Tree', 'Random Forest'], [accuracy_score(y_test, single_pred), accuracy_score(y_test, forest_pred)])
plt.ylabel('Accuracy')
plt.title('Performance Comparison')

plt.subplot(1, 2, 2)
# Show how prediction confidence varies
forest_proba = random_forest.predict_proba(X_test)
prediction_confidence = np.max(forest_proba, axis=1)
plt.hist(prediction_confidence, bins=20, alpha=0.7, color='skyblue')
plt.xlabel('Prediction Confidence')
plt.ylabel('Frequency')
plt.title('Random Forest Prediction Confidence Distribution')

plt.tight_layout()
plt.show()
```

## Custom Random Forest Implementation

```python
class SimpleRandomForest:
    def __init__(self, n_estimators=100, max_depth=3, max_features='sqrt', random_state=42):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.max_features = max_features
        self.random_state = random_state
        self.trees = []
        self.feature_indices = []

    def _bootstrap_sample(self, X, y):
        """Create bootstrap sample"""
        n_samples = X.shape[0]
        indices = np.random.choice(n_samples, size=n_samples, replace=True)
        return X[indices], y[indices]

    def _get_max_features(self, n_features):
        """Get number of features to consider"""
        if self.max_features == 'sqrt':
            return int(np.sqrt(n_features))
        elif self.max_features == 'log2':
            return int(np.log2(n_features))
        elif isinstance(self.max_features, int):
            return self.max_features
        else:
            return n_features

    def _build_tree(self, X, y):
        """Build a single decision tree"""
        from sklearn.tree import DecisionTreeClassifier

        # Get random subset of features
        n_features = X.shape[1]
        max_features = self._get_max_features(n_features)
        feature_indices = np.random.choice(n_features, size=max_features, replace=False)

        # Train tree on selected features
        tree = DecisionTreeClassifier(max_depth=self.max_depth, random_state=self.random_state)
        tree.fit(X[:, feature_indices], y)

        return tree, feature_indices

    def fit(self, X, y):
        """Train the random forest"""
        np.random.seed(self.random_state)

        self.trees = []
        self.feature_indices = []

        for i in range(self.n_estimators):
            # Bootstrap sample
            X_bootstrap, y_bootstrap = self._bootstrap_sample(X, y)

            # Build tree
            tree, feature_indices = self._build_tree(X_bootstrap, y_bootstrap)

            self.trees.append(tree)
            self.feature_indices.append(feature_indices)

        return self

    def predict(self, X):
        """Make predictions"""
        predictions = []

        for tree, feature_indices in zip(self.trees, self.feature_indices):
            pred = tree.predict(X[:, feature_indices])
            predictions.append(pred)

        # Majority vote
        predictions = np.array(predictions)
        final_predictions = []

        for i in range(X.shape[0]):
            vote_counts = np.bincount(predictions[:, i])
            final_predictions.append(np.argmax(vote_counts))

        return np.array(final_predictions)

    def predict_proba(self, X):
        """Get prediction probabilities"""
        probabilities = []

        for tree, feature_indices in zip(self.trees, self.feature_indices):
            proba = tree.predict_proba(X[:, feature_indices])
            probabilities.append(proba)

        # Average probabilities
        return np.mean(probabilities, axis=0)

# Test custom implementation
custom_rf = SimpleRandomForest(n_estimators=50, max_depth=5, random_state=42)
custom_rf.fit(X_train, y_train)
custom_pred = custom_rf.predict(X_test)

print(f"Custom Random Forest Accuracy: {accuracy_score(y_test, custom_pred):.3f}")
```

## Feature Importance in Random Forests

### Understanding Feature Importance

Random Forests provide robust feature importance measures based on how much each feature contributes to reducing impurity across all trees.

```python
# Feature importance analysis
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Get feature importances
feature_importance = rf_model.feature_importances_
feature_names = [f'Feature {i}' for i in range(X.shape[1])]

# Sort features by importance
indices = np.argsort(feature_importance)[::-1]

plt.figure(figsize=(12, 6))
plt.bar(range(len(feature_importance)), feature_importance[indices])
plt.xticks(range(len(feature_importance)), [feature_names[i] for i in indices], rotation=45)
plt.title('Random Forest Feature Importance')
plt.ylabel('Importance')
plt.show()

print("Top 5 most important features:")
for i in range(5):
    idx = indices[i]
    print(f"  {feature_names[idx]}: {feature_importance[idx]:.3f}")

# Compare with single tree importance
single_tree = DecisionTreeClassifier(random_state=42)
single_tree.fit(X_train, y_train)

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.bar(range(len(single_tree.feature_importances_)), single_tree.feature_importances_)
plt.title('Single Tree Feature Importance')

plt.subplot(1, 3, 2)
plt.bar(range(len(rf_model.feature_importances_)), rf_model.feature_importances_)
plt.title('Random Forest Feature Importance')

plt.subplot(1, 3, 3)
# Compare importance rankings
single_ranks = np.argsort(single_tree.feature_importances_)[::-1]
rf_ranks = np.argsort(rf_model.feature_importances_)[::-1]

plt.scatter(range(len(feature_importance)), single_ranks, alpha=0.7, label='Single Tree', s=50)
plt.scatter(range(len(feature_importance)), rf_ranks, alpha=0.7, label='Random Forest', s=50)
plt.xlabel('Feature Index')
plt.ylabel('Importance Rank')
plt.legend()
plt.title('Feature Importance Comparison')

plt.tight_layout()
plt.show()
```

### Permutation Importance

A more robust method for measuring feature importance:

```python
from sklearn.inspection import permutation_importance

# Calculate permutation importance
perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)

plt.figure(figsize=(12, 6))
indices = np.argsort(perm_importance.importances_mean)[::-1]
plt.bar(range(len(perm_importance.importances_mean)), perm_importance.importances_mean[indices])
plt.errorbar(range(len(perm_importance.importances_mean)),
             perm_importance.importances_mean[indices],
             yerr=perm_importance.importances_std[indices], fmt='none', capsize=5, alpha=0.7)
plt.xticks(range(len(feature_importance)), [feature_names[i] for i in indices], rotation=45)
plt.title('Permutation Feature Importance')
plt.ylabel('Importance (Decrease in Accuracy)')
plt.show()

print("Permutation Importance - Top 5 features:")
for i in range(5):
    idx = indices[i]
    importance = perm_importance.importances_mean[idx]
    std = perm_importance.importances_std[idx]
    print(f"  {feature_names[idx]}: {importance:.3f} ± {std:.3f}")
```

## Hyperparameter Tuning

### Key Parameters

```python
# Hyperparameter analysis
from sklearn.model_selection import validation_curve

# 1. Number of trees (n_estimators)
n_estimators = [10, 50, 100, 200, 500]
train_scores, val_scores = validation_curve(
    RandomForestClassifier(random_state=42), X_train, y_train,
    param_name='n_estimators', param_range=n_estimators, cv=5
)

plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
plt.plot(n_estimators, np.mean(train_scores, axis=1), 'o-', label='Training Score')
plt.plot(n_estimators, np.mean(val_scores, axis=1), 's-', label='Validation Score')
plt.xlabel('Number of Trees')
plt.ylabel('Accuracy')
plt.title('n_estimators Effect')
plt.legend()

# 2. Maximum depth
max_depths = [3, 5, 7, 10, 15, None]
train_scores, val_scores = validation_curve(
    RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train,
    param_name='max_depth', param_range=max_depths, cv=5
)

plt.subplot(2, 3, 2)
plt.plot([str(d) for d in max_depths], np.mean(train_scores, axis=1), 'o-', label='Training Score')
plt.plot([str(d) for d in max_depths], np.mean(val_scores, axis=1), 's-', label='Validation Score')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('max_depth Effect')
plt.xticks(rotation=45)
plt.legend()

# 3. Maximum features
max_features = ['sqrt', 'log2', 2, 3, 5, 7, 10]
train_scores, val_scores = validation_curve(
    RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train,
    param_name='max_features', param_range=max_features, cv=5
)

plt.subplot(2, 3, 3)
plt.plot([str(f) for f in max_features], np.mean(train_scores, axis=1), 'o-', label='Training Score')
plt.plot([str(f) for f in max_features], np.mean(val_scores, axis=1), 's-', label='Validation Score')
plt.xlabel('Max Features')
plt.ylabel('Accuracy')
plt.title('max_features Effect')
plt.xticks(rotation=45)
plt.legend()

# 4. Minimum samples per leaf
min_samples_leaf = [1, 2, 5, 10, 20]
train_scores, val_scores = validation_curve(
    RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train,
    param_name='min_samples_leaf', param_range=min_samples_leaf, cv=5
)

plt.subplot(2, 3, 4)
plt.plot(min_samples_leaf, np.mean(train_scores, axis=1), 'o-', label='Training Score')
plt.plot(min_samples_leaf, np.mean(val_scores, axis=1), 's-', label='Validation Score')
plt.xlabel('Min Samples per Leaf')
plt.ylabel('Accuracy')
plt.title('min_samples_leaf Effect')
plt.legend()

# 5. Minimum samples to split
min_samples_split = [2, 5, 10, 20, 50]
train_scores, val_scores = validation_curve(
    RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train,
    param_name='min_samples_split', param_range=min_samples_split, cv=5
)

plt.subplot(2, 3, 5)
plt.plot(min_samples_split, np.mean(train_scores, axis=1), 'o-', label='Training Score')
plt.plot(min_samples_split, np.mean(val_scores, axis=1), 's-', label='Validation Score')
plt.xlabel('Min Samples to Split')
plt.ylabel('Accuracy')
plt.title('min_samples_split Effect')
plt.legend()

# 6. Bootstrap vs. no bootstrap
bootstrap_options = [True, False]
train_scores, val_scores = validation_curve(
    RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train,
    param_name='bootstrap', param_range=bootstrap_options, cv=5
)

plt.subplot(2, 3, 6)
plt.bar(['Bootstrap=True', 'Bootstrap=False'],
        [np.mean(val_scores[0]), np.mean(val_scores[1])])
plt.ylabel('Validation Accuracy')
plt.title('Bootstrap Effect')

plt.tight_layout()
plt.show()
```

### Grid Search for Optimal Parameters

```python
from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200, 500],
    'max_depth': [5, 10, None],
    'max_features': ['sqrt', 'log2', 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5]
}

# Grid search
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score:", grid_search.best_score_)

# Evaluate on test set
best_rf = grid_search.best_estimator_
test_accuracy = best_rf.score(X_test, y_test)
print(f"Test accuracy: {test_accuracy:.3f}")

# Compare with default parameters
default_rf = RandomForestClassifier(random_state=42)
default_rf.fit(X_train, y_train)
default_accuracy = default_rf.score(X_test, y_test)

print(f"Default accuracy: {default_accuracy:.3f}")
print(f"Improvement: {test_accuracy - default_accuracy:.3f}")
```

## Out-of-Bag (OOB) Error

Random Forests can estimate generalization error without a separate validation set using out-of-bag samples.

```python
# OOB error analysis
oob_scores = []
n_estimators_range = range(10, 201, 10)

for n_est in n_estimators_range:
    rf_oob = RandomForestClassifier(n_estimators=n_est, oob_score=True, random_state=42)
    rf_oob.fit(X_train, y_train)
    oob_scores.append(rf_oob.oob_score_)

plt.figure(figsize=(10, 6))
plt.plot(n_estimators_range, oob_scores, 'o-', linewidth=2, label='OOB Score')
plt.axhline(y=best_rf.score(X_test, y_test), color='red', linestyle='--',
           label='Test Score (GridSearch)', alpha=0.7)
plt.xlabel('Number of Trees')
plt.ylabel('Accuracy')
plt.title('Out-of-Bag Error vs Number of Trees')
plt.legend()
plt.grid(True)
plt.show()

print(f"Final OOB Score: {oob_scores[-1]:.3f}")
```

## Real-World Applications

### Medical Diagnosis (Breast Cancer Dataset)

```python
# Real-world example with breast cancer dataset
cancer = load_breast_cancer()
X_cancer, y_cancer = cancer.data, cancer.target

X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X_cancer, y_cancer, test_size=0.2, random_state=42
)

# Train Random Forest
cancer_rf = RandomForestClassifier(n_estimators=200, random_state=42)
cancer_rf.fit(X_train_c, y_train_c)

# Evaluate
cancer_pred = cancer_rf.predict(X_test_c)
cancer_accuracy = accuracy_score(y_test_c, cancer_pred)

print(f"Breast Cancer Classification Accuracy: {cancer_accuracy:.3f}")

# Feature importance for medical diagnosis
cancer_importance = pd.DataFrame({
    'Feature': cancer.feature_names,
    'Importance': cancer_rf.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(12, 8))
plt.barh(range(15), cancer_importance['Importance'].head(15))
plt.yticks(range(15), cancer_importance['Feature'].head(15))
plt.xlabel('Feature Importance')
plt.title('Top 15 Features in Breast Cancer Diagnosis')
plt.tight_layout()
plt.show()

print("Top 5 most important features for cancer diagnosis:")
for i, row in cancer_importance.head().iterrows():
    print(f"  {row['Feature']}: {row['Importance']:.3f}")
```

### Financial Risk Assessment

```python
# Simulate financial risk assessment
np.random.seed(42)

# Customer financial features
credit_score = np.random.normal(650, 100, 1000)
income = np.random.normal(50000, 25000, 1000)
debt_ratio = np.random.exponential(0.3, 1000)
employment_years = np.random.gamma(5, 1, 1000)
loan_amount = np.random.normal(25000, 15000, 1000)

# Risk calculation
risk_score = (
    -0.002 * credit_score +
    -0.00002 * income +
    5 * debt_ratio +
    -0.1 * employment_years +
    0.00001 * loan_amount +
    np.random.normal(0, 0.5, 1000)
)

# Binary risk classification
risk = (risk_score > 0).astype(int)

# Train Random Forest for risk assessment
X_financial = np.column_stack([credit_score, income, debt_ratio, employment_years, loan_amount])
y_financial = risk

X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(
    X_financial, y_financial, test_size=0.2, random_state=42
)

financial_rf = RandomForestClassifier(n_estimators=100, random_state=42)
financial_rf.fit(X_train_f, y_train_f)

financial_pred = financial_rf.predict(X_test_f)
financial_accuracy = accuracy_score(y_test_f, financial_pred)

print(f"Financial Risk Assessment Accuracy: {financial_accuracy:.3f}")

# Feature importance for financial risk
financial_features = ['Credit Score', 'Income', 'Debt Ratio', 'Employment Years', 'Loan Amount']
financial_importance = financial_rf.feature_importances_

plt.figure(figsize=(10, 6))
plt.bar(financial_features, financial_importance)
plt.title('Feature Importance in Financial Risk Assessment')
plt.ylabel('Importance')
plt.xticks(rotation=45)
plt.show()

print("Feature importance for financial risk:")
for feature, importance in zip(financial_features, financial_importance):
    print(f"  {feature}: {importance:.3f}")
```

## Advanced Random Forest Techniques

### 1. Random Forest for Regression

```python
from sklearn.datasets import make_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Regression example
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# Train Random Forest for regression
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train_r, y_train_r)

# Predictions
y_pred_r = rf_regressor.predict(X_test_r)

# Evaluate
mse = mean_squared_error(y_test_r, y_pred_r)
r2 = r2_score(y_test_r, y_pred_r)

print(f"Regression - MSE: {mse:.3f}, R²: {r2:.3f}")

# Feature importance for regression
plt.figure(figsize=(12, 6))
plt.bar(range(len(rf_regressor.feature_importances_)), rf_regressor.feature_importances_)
plt.title('Random Forest Feature Importance (Regression)')
plt.ylabel('Importance')
plt.show()
```

### 2. Handling Imbalanced Data

```python
from imblearn.over_sampling import SMOTE

# Create imbalanced dataset
X_imb = np.vstack([X[y==0][:100], X[y==1]])
y_imb = np.hstack([y[y==0][:100], y[y==1]])

print(f"Imbalanced dataset - Class 0: {np.sum(y_imb==0)}, Class 1: {np.sum(y_imb==1)}")

# Train Random Forest with class weighting
rf_balanced = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_balanced.fit(X_imb, y_imb)

# Use SMOTE for oversampling
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_imb, y_imb)

rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)
rf_smote.fit(X_resampled, y_resampled)

print(f"After SMOTE - Class 0: {np.sum(y_resampled==0)}, Class 1: {np.sum(y_resampled==1)}")

# Compare approaches
X_test_imb, y_test_imb = X_test, y_test  # Use original test set
print(f"Balanced RF Accuracy: {rf_balanced.score(X_test_imb, y_test_imb):.3f}")
print(f"SMOTE RF Accuracy: {rf_smote.score(X_test_imb, y_test_imb):.3f}")
```

### 3. Partial Dependence Plots

```python
from sklearn.inspection import partial_dependence, PartialDependenceDisplay

# Create Partial Dependence Plots
fig, ax = plt.subplots(2, 2, figsize=(15, 10))

# Single feature partial dependence
for i, feature_idx in enumerate([0, 1, 2, 3]):
    row = i // 2
    col = i % 2

    pd_result = partial_dependence(rf_model, X_train, [feature_idx])
    ax[row, col].plot(pd_result['values'][0], pd_result['average'][0])
    ax[row, col].set_xlabel(f'Feature {feature_idx}')
    ax[row, col].set_ylabel('Partial Dependence')
    ax[row, col].set_title(f'Feature {feature_idx} Partial Dependence')

plt.tight_layout()
plt.show()

# Two-feature interaction
fig, ax = plt.subplots(1, 1, figsize=(8, 6))
display = PartialDependenceDisplay.from_estimator(
    rf_model, X_train, [[0, 1]], ax=ax
)
plt.title('Feature 0 vs Feature 1 Interaction')
plt.show()
```

## Common Pitfalls and Solutions

### 1. Overfitting
**Problem**: Too many trees or trees that are too deep
**Solutions**:
- Limit tree depth
- Increase minimum samples per leaf
- Use cross-validation for hyperparameter tuning

### 2. High Variance in Predictions
**Problem**: Random Forest predictions can vary between runs
**Solutions**:
- Set random_state for reproducibility
- Use more trees to reduce variance
- Consider using bootstrap=False for stability

### 3. Memory Usage
**Problem**: Random Forests can consume significant memory
**Solutions**:
- Limit number of trees
- Use max_depth to control tree size
- Consider using ExtraTreesClassifier for faster training

### 4. Feature Scaling
**Problem**: Random Forests are not affected by feature scaling, but it can impact some implementations
**Solutions**:
- Generally no scaling needed
- Focus on feature engineering instead

## Key Takeaways

1. **Ensemble Power**: Random Forests leverage the wisdom of crowds for better predictions
2. **Robustness**: Less prone to overfitting than individual decision trees
3. **Feature Importance**: Provides reliable feature importance measures
4. **No Scaling Needed**: Works well with features on different scales
5. **Handles Missing Data**: Can handle missing values naturally
6. **Versatility**: Works for both classification and regression
7. **Hyperparameter Sensitivity**: Performance depends on proper parameter tuning

### When to Use Random Forests:
- When you need a robust, high-accuracy model
- When interpretability is important (feature importance)
- When dealing with mixed data types
- When you have many features
- As a baseline before trying more complex models

### When to Avoid Random Forests:
- When you need real-time predictions (can be slow)
- When you have very high-dimensional sparse data
- When you need probability calibration
- When model size is a constraint

Random Forests remain one of the most reliable and widely-used machine learning algorithms, providing an excellent balance of accuracy, robustness, and interpretability.
