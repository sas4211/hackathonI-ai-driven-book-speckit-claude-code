---
sidebar_position: 4
---

# Decision Trees

Decision Trees are powerful and intuitive machine learning algorithms that model decisions as a tree-like structure. They're among the most interpretable machine learning models, making them excellent for understanding how predictions are made.

## What are Decision Trees?

Decision Trees work by recursively splitting the data into subsets based on feature values, creating a tree structure where:
- **Internal nodes** represent feature tests
- **Branches** represent the outcomes of those tests
- **Leaf nodes** represent final predictions

### How Decision Trees Work

1. **Start at the root**: Begin with all training examples
2. **Find the best split**: Choose the feature and threshold that best separates classes
3. **Split the data**: Divide examples into subsets based on the split
4. **Repeat recursively**: Apply the same process to each subset
5. **Stop when criteria are met**: When purity is achieved or other stopping conditions

## Building Decision Trees

### Information Gain and Entropy

Decision trees use **entropy** to measure impurity and **information gain** to determine the best splits.

#### Entropy Formula
```
Entropy(S) = -∑(p_i * log₂(p_i))
```

Where p_i is the proportion of examples belonging to class i.

#### Information Gain
```
IG(S, A) = Entropy(S) - ∑(|S_v|/|S| * Entropy(S_v))
```

Where A is the attribute being tested, and S_v are the subsets created by splitting on A.

### Basic Implementation

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,
                          n_informative=2, n_clusters_per_class=1, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train decision tree
tree_model = DecisionTreeClassifier(random_state=42)
tree_model.fit(X_train, y_train)

# Make predictions
y_pred = tree_model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.3f}")

# Visualize the tree
plt.figure(figsize=(20, 10))
plot_tree(tree_model, filled=True, feature_names=['Feature 1', 'Feature 2'],
          class_names=['Class 0', 'Class 1'], fontsize=10)
plt.title('Decision Tree Visualization')
plt.show()
```

### Custom Decision Tree Implementation

```python
class SimpleDecisionTree:
    def __init__(self, max_depth=3, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def entropy(self, y):
        """Calculate entropy of a dataset"""
        if len(y) == 0:
            return 0
        proportions = np.bincount(y) / len(y)
        return -np.sum(p * np.log2(p) for p in proportions if p > 0)

    def information_gain(self, X, y, feature_idx, threshold):
        """Calculate information gain for a split"""
        left_mask = X[:, feature_idx] <= threshold
        right_mask = ~left_mask

        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
            return 0

        left_entropy = self.entropy(y[left_mask])
        right_entropy = self.entropy(y[right_mask])

        n = len(y)
        weighted_entropy = (np.sum(left_mask) / n) * left_entropy + (np.sum(right_mask) / n) * right_entropy

        return self.entropy(y) - weighted_entropy

    def find_best_split(self, X, y):
        """Find the best feature and threshold for splitting"""
        best_gain = 0
        best_feature = None
        best_threshold = None

        n_features = X.shape[1]

        for feature_idx in range(n_features):
            thresholds = np.unique(X[:, feature_idx])
            for threshold in thresholds:
                gain = self.information_gain(X, y, feature_idx, threshold)
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold

        return best_gain, best_feature, best_threshold

    def build_tree(self, X, y, depth=0):
        """Recursively build the decision tree"""
        # Check stopping criteria
        if (depth >= self.max_depth or
            len(np.unique(y)) == 1 or
            len(y) < self.min_samples_split):
            return {'prediction': np.bincount(y).argmax()}

        # Find best split
        gain, feature, threshold = self.find_best_split(X, y)

        if gain == 0:
            return {'prediction': np.bincount(y).argmax()}

        # Split the data
        left_mask = X[:, feature] <= threshold
        right_mask = ~left_mask

        # Recursively build subtrees
        left_subtree = self.build_tree(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self.build_tree(X[right_mask], y[right_mask], depth + 1)

        return {
            'feature': feature,
            'threshold': threshold,
            'left': left_subtree,
            'right': right_subtree
        }

    def fit(self, X, y):
        """Train the decision tree"""
        self.tree = self.build_tree(X, y)

    def predict_sample(self, sample, tree):
        """Make prediction for a single sample"""
        if 'prediction' in tree:
            return tree['prediction']

        feature = tree['feature']
        threshold = tree['threshold']

        if sample[feature] <= threshold:
            return self.predict_sample(sample, tree['left'])
        else:
            return self.predict_sample(sample, tree['right'])

    def predict(self, X):
        """Make predictions for multiple samples"""
        return np.array([self.predict_sample(sample, self.tree) for sample in X])

# Test our custom implementation
custom_tree = SimpleDecisionTree(max_depth=3)
custom_tree.fit(X_train, y_train)
custom_pred = custom_tree.predict(X_test)

print(f"Custom tree accuracy: {accuracy_score(y_test, custom_pred):.3f}")
```

## Decision Tree Visualization and Interpretation

### Tree Structure Analysis

```python
# Analyze tree structure
tree_model = DecisionTreeClassifier(max_depth=4, random_state=42)
tree_model.fit(X_train, y_train)

# Get tree properties
n_nodes = tree_model.tree_.node_count
children_left = tree_model.tree_.children_left
children_right = tree_model.tree_.children_right
feature = tree_model.tree_.feature
threshold = tree_model.tree_.threshold

print(f"Number of nodes: {n_nodes}")
print(f"Max depth: {tree_model.get_depth()}")
print(f"Number of leaves: {tree_model.get_n_leaves()}")

# Visualize decision boundaries
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.6)
plt.title('Data Distribution')

plt.subplot(1, 3, 2)
# Plot decision boundaries
xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),
                     np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100))
grid = np.c_[xx.ravel(), yy.ravel()]
Z = tree_model.predict(grid).reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black')
plt.title('Decision Tree Boundaries')

plt.subplot(1, 3, 3)
plot_tree(tree_model, filled=True, feature_names=['Feature 1', 'Feature 2'],
          class_names=['Class 0', 'Class 1'], fontsize=8)
plt.title('Tree Structure')

plt.tight_layout()
plt.show()
```

### Feature Importance

```python
# Feature importance analysis
feature_importance = tree_model.feature_importances_
feature_names = ['Feature 1', 'Feature 2']

plt.figure(figsize=(8, 6))
plt.bar(feature_names, feature_importance)
plt.title('Feature Importance in Decision Tree')
plt.ylabel('Importance')
plt.show()

print("Feature importance:")
for i, importance in enumerate(feature_importance):
    print(f"  {feature_names[i]}: {importance:.3f}")
```

## Handling Different Data Types

### Categorical Features

```python
# Example with categorical features
from sklearn.preprocessing import LabelEncoder

# Create dataset with categorical features
data = {
    'age': ['young', 'young', 'middle', 'middle', 'old', 'old'],
    'income': ['high', 'high', 'high', 'medium', 'medium', 'low'],
    'student': ['no', 'no', 'no', 'no', 'yes', 'yes'],
    'credit_rating': ['fair', 'excellent', 'fair', 'fair', 'fair', 'excellent'],
    'buys_computer': ['no', 'no', 'yes', 'yes', 'yes', 'no']
}

# Convert to numerical
df = pd.DataFrame(data)
le = LabelEncoder()
for column in df.columns[:-1]:  # Exclude target
    df[column] = le.fit_transform(df[column])

X_cat = df.iloc[:, :-1].values
y_cat = df.iloc[:, -1].values

# Train decision tree
cat_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)
cat_tree.fit(X_cat, y_cat)

# Visualize
plt.figure(figsize=(15, 8))
plot_tree(cat_tree, filled=True,
          feature_names=['Age', 'Income', 'Student', 'Credit'],
          class_names=['No', 'Yes'], fontsize=10)
plt.title('Decision Tree with Categorical Features')
plt.show()
```

### Multi-class Classification

```python
# Multi-class example with Iris dataset
iris = load_iris()
X_iris, y_iris = iris.data, iris.target

X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(
    X_iris, y_iris, test_size=0.2, random_state=42
)

# Train multi-class decision tree
iris_tree = DecisionTreeClassifier(random_state=42)
iris_tree.fit(X_train_iris, y_train_iris)

# Evaluate
iris_pred = iris_tree.predict(X_test_iris)
print(f"Iris classification accuracy: {accuracy_score(y_test_iris, iris_pred):.3f}")

# Confusion matrix
cm = confusion_matrix(y_test_iris, iris_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.title('Confusion Matrix - Iris Dataset')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
```

## Tree Pruning and Overfitting

### Pre-pruning (Setting Constraints)

```python
# Different tree depths to prevent overfitting
depths = [1, 2, 3, 4, 5, 10, None]
train_accuracies = []
test_accuracies = []

for depth in depths:
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_train, y_train)

    train_acc = tree.score(X_train, y_train)
    test_acc = tree.score(X_test, y_test)

    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)

    print(f"Depth {depth}: Train={train_acc:.3f}, Test={test_acc:.3f}")

# Plot overfitting
plt.figure(figsize=(10, 6))
plt.plot(depths, train_accuracies, 'o-', label='Training Accuracy', linewidth=2)
plt.plot(depths, test_accuracies, 's-', label='Test Accuracy', linewidth=2)
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('Overfitting in Decision Trees')
plt.legend()
plt.grid(True)
plt.show()
```

### Post-pruning (Cost Complexity Pruning)

```python
# Cost complexity pruning
tree = DecisionTreeClassifier(random_state=42)
path = tree.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# Train trees with different alpha values
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

# Find best alpha
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]

optimal_alpha = ccp_alphas[np.argmax(test_scores)]
print(f"Optimal alpha: {optimal_alpha:.6f}")

# Plot pruning results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(ccp_alphas, train_scores, 'o-', label='Training Accuracy')
plt.plot(ccp_alphas, test_scores, 's-', label='Test Accuracy')
plt.axvline(x=optimal_alpha, color='red', linestyle='--', alpha=0.7)
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Accuracy')
plt.title('Cost Complexity Pruning')
plt.legend()
plt.xscale('log')

plt.subplot(1, 2, 2)
node_counts = [clf.tree_.node_count for clf in clfs]
plt.plot(ccp_alphas, node_counts, 'o-')
plt.axvline(x=optimal_alpha, color='red', linestyle='--', alpha=0.7)
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Number of Nodes')
plt.title('Tree Complexity vs Alpha')
plt.xscale('log')

plt.tight_layout()
plt.show()
```

## Advanced Decision Tree Features

### Feature Selection and Importance

```python
# Feature importance with more features
X_multi, y_multi = make_classification(n_samples=1000, n_features=10, n_informative=5,
                                     n_redundant=2, random_state=42)

X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)

tree_multi = DecisionTreeClassifier(random_state=42, max_depth=5)
tree_multi.fit(X_train_m, y_train_m)

# Feature importance
feature_importance = tree_multi.feature_importances_
feature_names = [f'Feature {i}' for i in range(X_multi.shape[1])]

# Plot importance
plt.figure(figsize=(12, 6))
indices = np.argsort(feature_importance)[::-1]
plt.bar(range(len(feature_importance)), feature_importance[indices])
plt.xticks(range(len(feature_importance)), [feature_names[i] for i in indices], rotation=45)
plt.title('Feature Importance in Decision Tree')
plt.ylabel('Importance')
plt.show()

print("Top 5 most important features:")
for i in range(5):
    idx = indices[i]
    print(f"  {feature_names[idx]}: {feature_importance[idx]:.3f}")
```

### Decision Tree Rules Extraction

```python
from sklearn.tree import export_text

# Extract rules from decision tree
tree_rules = export_text(tree_model, feature_names=['Feature_1', 'Feature_2'])
print("Decision Tree Rules:")
print(tree_rules)

# Custom function to extract more readable rules
def extract_rules(tree, feature_names):
    """Extract human-readable rules from decision tree"""
    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != -2 else "undefined!"
        for i in tree_.feature
    ]

    def recurse(node, depth):
        indent = "  " * depth
        if tree_.feature[node] != -2:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            print(f"{indent}if {name} <= {threshold:.3f}:")
            recurse(tree_.children_left[node], depth + 1)
            print(f"{indent}else:  # if {name} > {threshold:.3f}")
            recurse(tree_.children_right[node], depth + 1)
        else:
            class_counts = tree_.value[node][0]
            class_idx = np.argmax(class_counts)
            print(f"{indent}return class {class_idx}")

print("\nHuman-readable rules:")
extract_rules(tree_model, ['Feature_1', 'Feature_2'])
```

## Ensemble Methods: Random Forest Connection

### Understanding Random Forest Foundation

```python
from sklearn.ensemble import RandomForestClassifier

# Compare single tree vs random forest
single_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)

single_tree.fit(X_train, y_train)
random_forest.fit(X_train, y_train)

single_pred = single_tree.predict(X_test)
forest_pred = random_forest.predict(X_test)

print(f"Single Tree Accuracy: {accuracy_score(y_test, single_pred):.3f}")
print(f"Random Forest Accuracy: {accuracy_score(y_test, forest_pred):.3f}")

# Feature importance comparison
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Single tree importance
ax1.bar(range(len(tree_model.feature_importances_)), tree_model.feature_importances_)
ax1.set_title('Single Decision Tree Feature Importance')
ax1.set_ylabel('Importance')

# Random forest importance
ax2.bar(range(len(random_forest.feature_importances_)), random_forest.feature_importances_)
ax2.set_title('Random Forest Feature Importance')
ax2.set_ylabel('Importance')

plt.tight_layout()
plt.show()
```

## Real-World Applications

### Medical Diagnosis

```python
# Simulate medical diagnosis with decision tree
np.random.seed(42)

# Patient data
age = np.random.normal(50, 20, 1000)
blood_pressure = np.random.normal(120, 25, 1000)
cholesterol = np.random.normal(200, 50, 1000)
diabetes = np.random.choice([0, 1], 1000, p=[0.7, 0.3])

# Disease risk calculation
disease_risk = (
    0.01 * age +
    0.005 * blood_pressure +
    0.003 * cholesterol +
    20 * diabetes +
    np.random.normal(0, 10, 1000)
)

disease = (disease_risk > 50).astype(int)

# Train decision tree
X_med = np.column_stack([age, blood_pressure, cholesterol, diabetes])
y_med = disease

X_train_med, X_test_med, y_train_med, y_test_med = train_test_split(X_med, y_med, test_size=0.2, random_state=42)

med_tree = DecisionTreeClassifier(max_depth=4, random_state=42)
med_tree.fit(X_train_med, y_train_med)

# Evaluate
med_pred = med_tree.predict(X_test_med)
print(f"Medical diagnosis accuracy: {accuracy_score(y_test_med, med_pred):.3f}")

# Feature importance for medical diagnosis
med_features = ['Age', 'Blood Pressure', 'Cholesterol', 'Diabetes']
med_importance = med_tree.feature_importances_

plt.figure(figsize=(10, 6))
plt.bar(med_features, med_importance)
plt.title('Feature Importance in Medical Diagnosis')
plt.ylabel('Importance')
plt.show()

print("Medical diagnosis feature importance:")
for feature, importance in zip(med_features, med_importance):
    print(f"  {feature}: {importance:.3f}")
```

### Customer Segmentation

```python
# Customer segmentation example
np.random.seed(123)

# Customer features
income = np.random.normal(50000, 20000, 1000)
age = np.random.normal(35, 12, 1000)
spending_score = np.random.normal(50, 25, 1000)
loyalty_years = np.random.exponential(3, 1000)

# Customer category (based on spending patterns)
category_score = (
    0.5 * (income > 60000) +
    0.3 * (spending_score > 70) +
    0.2 * (loyalty_years > 5) +
    np.random.normal(0, 0.5, 1000)
)

customer_category = np.digitize(category_score, [-0.5, 0.5, 1.5])
customer_category = np.clip(customer_category, 0, 3)  # 0-3 categories

# Train decision tree for customer segmentation
X_cust = np.column_stack([income, age, spending_score, loyalty_years])
y_cust = customer_category

X_train_cust, X_test_cust, y_train_cust, y_test_cust = train_test_split(
    X_cust, y_cust, test_size=0.2, random_state=42
)

cust_tree = DecisionTreeClassifier(max_depth=4, random_state=42)
cust_tree.fit(X_train_cust, y_train_cust)

# Evaluate
cust_pred = cust_tree.predict(X_test_cust)
print(f"Customer segmentation accuracy: {accuracy_score(y_test_cust, cust_pred):.3f}")

# Visualize tree for customer segmentation
plt.figure(figsize=(20, 10))
plot_tree(cust_tree, filled=True,
          feature_names=['Income', 'Age', 'Spending Score', 'Loyalty Years'],
          class_names=['Low Value', 'Medium Value', 'High Value', 'Premium'],
          fontsize=10)
plt.title('Customer Segmentation Decision Tree')
plt.show()
```

## Common Pitfalls and Solutions

### 1. Overfitting
**Problem**: Deep trees can memorize training data.
**Solutions**:
- Limit tree depth
- Set minimum samples per leaf
- Use cost complexity pruning
- Ensemble methods (Random Forests)

### 2. Instability
**Problem**: Small changes in data can lead to very different trees.
**Solutions**:
- Use ensemble methods
- Cross-validation for hyperparameter tuning
- Bootstrap aggregating

### 3. Bias towards features with more levels
**Problem**: Categorical features with many levels get preferential treatment.
**Solutions**:
- Use proper encoding
- Feature engineering
- Consider alternative algorithms

### 4. Handling missing values
**Problem**: Standard decision trees don't handle missing data well.
**Solutions**:
- Impute missing values
- Use specialized algorithms that handle missing data
- Create separate branches for missing values

## Key Takeaways

1. **Interpretability**: Decision trees are highly interpretable - you can see exactly how predictions are made
2. **No preprocessing**: Handle mixed data types and don't require feature scaling
3. **Non-linear relationships**: Can capture complex interactions between features
4. **Overfitting risk**: Prone to overfitting without proper constraints
5. **Instability**: Small data changes can lead to very different trees
6. **Ensemble foundation**: Basis for powerful ensemble methods like Random Forests
7. **Feature importance**: Naturally provides feature importance scores

Decision trees are excellent for:
- Exploratory data analysis
- Understanding feature relationships
- Creating interpretable models
- Building foundation for ensemble methods
- Handling heterogeneous data types

They're less suitable for:
- Very high-dimensional data
- When interpretability isn't needed
- When maximum predictive accuracy is the only goal
