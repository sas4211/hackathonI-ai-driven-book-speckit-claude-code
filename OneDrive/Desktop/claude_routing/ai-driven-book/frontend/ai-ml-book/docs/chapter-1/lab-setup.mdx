---
sidebar_position: 4
---

# Lab Setup: AI Development Environment

Welcome to the AI & Machine Learning lab! This chapter provides everything you need to set up a complete development environment for hands-on experimentation with AI concepts.

## What You'll Need

Before we dive into coding, let's ensure you have the right tools installed and configured.

### Option 1: Local Development Environment (Recommended for Serious Learning)

#### 1. Python Installation

**Why Python?** Python is the most popular language for AI/ML due to its simplicity and rich ecosystem of libraries.

**Installation:**
- **Windows**: Download from [python.org](https://python.org) (Python 3.8+ recommended)
- **Mac**: Use [Homebrew](https://brew.sh): `brew install python`
- **Linux**: Usually pre-installed, or use package manager

**Verify Installation:**
```bash
python --version
# Should show: Python 3.8.x or higher
```

#### 2. Virtual Environment Setup

Virtual environments keep your projects isolated and prevent package conflicts.

```bash
# Create a virtual environment
python -m venv ai-env

# Activate it
# Windows:
ai-env\Scripts\activate
# Mac/Linux:
source ai-env/bin/activate

# Verify activation
which python  # Should show path to your virtual environment
```

#### 3. Essential Python Libraries

Install the core AI/ML libraries:

```bash
pip install numpy pandas matplotlib seaborn scikit-learn jupyter
pip install tensorflow torch torchvision  # Deep learning frameworks
pip install plotly bokeh  # Interactive visualization
pip install requests beautifulsoup4  # Web scraping (for data collection)
```

**Quick Test:**
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
print("All libraries imported successfully!")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"Scikit-learn version: {sklearn.__version__}")
```

### Option 2: Cloud-Based Environment (No Installation Required)

#### Google Colab (Free)
- **URL**: [colab.research.google.com](https://colab.research.google.com)
- **Pros**: No installation, free GPU access, cloud storage
- **Cons**: Limited session time, requires Google account

#### Kaggle Notebooks (Free)
- **URL**: [kaggle.com/code](https://kaggle.com/code)
- **Pros**: Free GPU/TPU, datasets available, community
- **Cons**: Requires Kaggle account

#### Azure Notebooks (Free tier available)
- **URL**: [notebooks.azure.com](https://notebooks.azure.com)
- **Pros**: Microsoft ecosystem integration
- **Cons**: Limited free tier

### Option 3: Pre-configured Development Environment

#### Anaconda Distribution
Download from [anaconda.com](https://anaconda.com/products/distribution)

**Benefits:**
- All AI/ML packages pre-installed
- Package management made easy
- Jupyter notebooks included
- Works across platforms

**Quick Start:**
```bash
# Create an AI environment
conda create -n ai-env python=3.9
conda activate ai-env

# Install packages
conda install numpy pandas matplotlib scikit-learn jupyter
conda install -c conda-forge tensorflow pytorch
```

## Jupyter Notebooks: Your AI Playground

Jupyter notebooks are interactive documents that combine code, text, and visualizations - perfect for AI experimentation.

### Starting Jupyter

```bash
# From your virtual environment
jupyter notebook
# or
jupyter lab  # More modern interface
```

This opens your browser with the Jupyter interface. Create a new notebook to start coding!

### Jupyter Basics

**Notebook Structure:**
- **Cells**: Individual blocks of code or text
- **Code cells**: Execute Python code
- **Markdown cells**: Write explanations and documentation

**Essential Commands:**
- `Shift + Enter`: Run current cell and move to next
- `Ctrl + Enter`: Run current cell and stay
- `A`: Insert cell above
- `B`: Insert cell below
- `M`: Convert to Markdown cell
- `Y`: Convert to Code cell

### Example Notebook Setup

Create a new notebook and test your setup:

```python
# Cell 1: Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

print("✅ All imports successful!")

# Cell 2: Create sample data
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,
                          n_informative=2, random_state=42, n_clusters_per_class=1)

df = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])
df['Target'] = y

print(f"Dataset shape: {df.shape}")
print(f"Target distribution: {df['Target'].value_counts().to_dict()}")

# Cell 3: Visualize data
plt.figure(figsize=(10, 6))
scatter = plt.scatter(df['Feature_1'], df['Feature_2'], c=df['Target'], cmap='viridis', alpha=0.6)
plt.colorbar(scatter)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Sample Classification Data')
plt.show()

# Cell 4: Train a simple model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Model accuracy: {accuracy:.3f}")
```

## Essential Development Tools

### Code Editors and IDEs

#### VS Code (Recommended)
- **Extensions for AI/ML:**
  - Python
  - Jupyter
  - Pylance (IntelliSense)
  - GitLens
  - Python Indent

#### PyCharm
- Professional IDE for Python development
- Excellent debugging and refactoring tools
- Community edition available (free)

#### Sublime Text / Atom
- Lightweight editors
- Good for quick scripts and editing

### Git and Version Control

Track your AI experiments and collaborate with others:

```bash
# Initialize repository
git init

# Configure user
git config user.name "Your Name"
git config user.email "your.email@example.com"

# Add and commit files
git add .
git commit -m "Initial AI lab setup"

# Create remote repository (optional)
git remote add origin https://github.com/username/ai-lab.git
git push -u origin main
```

### Package Management

Keep your environment clean and reproducible:

```bash
# Create requirements.txt
pip freeze > requirements.txt

# Install from requirements
pip install -r requirements.txt

# Update packages
pip install --upgrade package_name
```

## GPU Acceleration (Optional but Recommended)

For deep learning, GPU acceleration can speed up training by 10-100x.

### NVIDIA CUDA Setup

1. **Check GPU compatibility**: Visit [nvidia.com/cuda-gpus](https://nvidia.com/cuda-gpus)
2. **Install CUDA Toolkit**: Download from [NVIDIA](https://developer.nvidia.com/cuda-downloads)
3. **Install cuDNN**: Download from [NVIDIA Developer](https://developer.nvidia.com/cudnn)

### TensorFlow GPU Setup

```bash
# Install GPU version
pip install tensorflow-gpu

# Verify GPU availability
import tensorflow as tf
print("GPUs Available: ", tf.config.list_physical_devices('GPU'))
```

### PyTorch GPU Setup

```bash
# Install with CUDA support
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118

# Verify GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
```

## Data Sources for Practice

### Built-in Datasets

```python
from sklearn.datasets import load_iris, load_boston, load_digits

# Iris dataset (classification)
iris = load_iris()
X, y = iris.data, iris.target

# Boston housing (regression)
boston = load_boston()
X, y = boston.data, boston.target

# Digits dataset (image classification)
digits = load_digits()
X, y = digits.data, digits.target
```

### Downloadable Datasets

```python
# Download larger datasets
from sklearn.datasets import fetch_openml

# MNIST handwritten digits
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist.data, mnist.target

# CIFAR-10 images
# Note: Requires additional download
```

### Kaggle Datasets

1. Create account at [kaggle.com](https://kaggle.com)
2. Download datasets for practice
3. Use `kaggle-cli` for command-line access

## Common Setup Issues and Solutions

### Issue 1: Package Conflicts
**Solution**: Use virtual environments or conda environments

```bash
# Create isolated environment
conda create -n my-ai-env python=3.9
conda activate my-ai-env
```

### Issue 2: Import Errors
**Solution**: Check Python path and package installation

```python
import sys
print(sys.path)

# Check installed packages
import pkg_resources
installed_packages = [d.project_name for d in pkg_resources.working_set]
print(installed_packages)
```

### Issue 3: Jupyter Not Finding Packages
**Solution**: Install packages in the correct environment

```bash
# Activate your environment first, then:
pip install package_name
# OR
conda install package_name
```

### Issue 4: GPU Not Detected
**Solutions**:
1. Check CUDA installation: `nvcc --version`
2. Verify GPU drivers are up to date
3. Install correct TensorFlow/PyTorch GPU version
4. Check environment variables for CUDA paths

## Lab Organization

### Project Structure

Organize your AI experiments systematically:

```
ai-lab/
├── 01-foundations/
│   ├── 01_data_exploration.ipynb
│   ├── 02_linear_regression.ipynb
│   └── 03_classification.ipynb
├── 02-deep-learning/
│   ├── 01_neural_networks.ipynb
│   ├── 02_cnn.ipynb
│   └── 03_rnn.ipynb
├── 03-advanced-topics/
├── data/
│   ├── sample_datasets/
│   └── external_datasets/
├── models/
│   ├── saved_models/
│   └── model_checkpoints/
├── utils/
│   ├── data_utils.py
│   └── visualization.py
└── requirements.txt
```

### Best Practices

1. **Version your notebooks**: Use Git for tracking changes
2. **Document your experiments**: Add comments and markdown explanations
3. **Save intermediate results**: Export plots, save model weights
4. **Use meaningful names**: For files, variables, and functions
5. **Clean up imports**: Remove unused imports regularly

## Quick Start Checklist

- [ ] Python 3.8+ installed
- [ ] Virtual environment created and activated
- [ ] Essential packages installed (numpy, pandas, matplotlib, scikit-learn)
- [ ] Jupyter notebook/lab working
- [ ] First notebook created and tested
- [ ] Git repository initialized (optional)
- [ ] GPU setup completed (if available and needed)

## Next Steps

Once your environment is set up:

1. **Start with simple examples**: Try the sample code above
2. **Experiment with datasets**: Load and explore different datasets
3. **Build your first model**: Train a simple classifier or regressor
4. **Visualize results**: Create plots and charts to understand your data
5. **Document your journey**: Keep track of what you learn

## Troubleshooting Resources

- **Official Documentation**: Always check the official docs first
- **Stack Overflow**: Search for specific error messages
- **GitHub Issues**: Check for known problems with libraries
- **Community Forums**: Reddit r/MachineLearning, Kaggle forums
- **Official Channels**: TensorFlow, PyTorch, scikit-learn communities

## Lab Safety and Best Practices

### Computational Resources
- **Monitor memory usage**: Large datasets can exhaust RAM
- **Save your work frequently**: Jupyter sessions can timeout
- **Use checkpoints**: Save model states during long training

### Code Quality
- **Write readable code**: Use meaningful variable names
- **Add comments**: Explain complex algorithms and decisions
- **Test your code**: Verify results make sense
- **Version control**: Track changes and experiments

### Data Ethics
- **Respect privacy**: Don't use sensitive personal data without permission
- **Check for bias**: Be aware of potential biases in datasets
- **Document sources**: Keep track of where your data comes from

You're now ready to start your AI journey! The lab environment is set up, and you have all the tools you need to experiment, learn, and build amazing AI applications.
